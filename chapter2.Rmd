---
title: "Blair's awesome notebook"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---
# Chapter 2: Reading Data



This is the code so the whole page reads the dataset
```{r readdata,echo=TRUE,results='hide',message=FALSE,warning=FALSE}
learning2014<-read.table("D:\\IODS-project\\IODS-project\\Data\\learning2014.csv")
```

#Description of the data set:
```
str(learning2014)
```
**N=166** with *7 variables (gender, age, attitude, deep, stra, surf and points.* 

**Gender** is a a dichotomous variable meaning it can have either female ("F"= female) or male ("M"= male). 

**Age** is a continous variable with positive numbers. 

**Attitude** is based off of the "global attitude towards statistics". 

**Deep**, **stra** (structure), **surf** (surface) are variables based on the *mean* of likert scores (1-5) to clustered questions.

**Points** represent the exam points.

#Graphical display of the data:
```{r pairplot}
pairs(learning2014[-1], col = learning2014$gender)
```

and a different way to look at the data:

``` {r pairplot1}
library(GGally)
library(ggplot2)

p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

p
```
Info about the tables:
Age does not have a normal distribution with the potential for many outliers (observations that are abnormal to the rest of the observations, in this case a few older persons) for both males and females. Other outliers can be seen in deep and Points. Points appears to be skewed to the left for both genders. 

##Regression Model:
The highest correlation is between *attitude* and *Points*, **Cor:** `r cor(learning2014$attitude,learning2014$Points)`.

Here is a visualization of the correlation:

```{r}
library(ggplot2)
qplot(attitude, Points, data = learning2014) + geom_smooth(method = "lm")

```

##Multiple regression:
To find the best association between the different variables, try adding more than one explainitory variable to the linear model. The correlation result will till you which one is a good fit.

```{r, results='asis'}

my_model2 <- lm(Points ~ attitude, data = learning2014)
results<-summary(my_model2)
knitr::kable(results$coefficients, digits=3, caption="Regression coefficients")
```

```{r, results='asis'}
my_model3 <- lm(Points ~ attitude + deep, data = learning2014)
results<-summary(my_model3)
knitr::kable(results$coefficients, digits=3, caption="Regression coefficients")
```

```{r, results='asis'}
my_model4 <- lm(Points ~ attitude + stra, data = learning2014)
results<-summary(my_model4)
knitr::kable(results$coefficients, digits=3, caption="Regression coefficients")
```

When looking that the estimates, only **attitude** seems to be a good fit. Both **deep** and **stra** have estimates close to zero and are not statiistically significant (p-value< 0.05). 

#Summary of fitted plot:
The Y-intercept of Points is around 11.6 and for every increase in 1 point on the exam it is predicted that attitude will also increase by 3.5. 

Intercept as well as attitude are statistically significant predictors. 
Coefficient of determination $R^2$ = `r results$r.squared` that is not particularly high.
Attitude can only predict some of the Points, but by adding more predicting variables we may be able to get a better fit for our model.


#Diagnostic plots:
Produce the following diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage. Explain the assumptions of the model and interpret the validity of those assumptions based on the diagnostic plots. (0-3 points)

```{r fig5, fig.path="figures/"}
my_model5 <- lm(Points ~ attitude, data = learning2014)
par(mfrow = c(2,2))
plot(my_model5, which = c(1))
```

**Residual vs fitted plot** tests the assumptions of wether the relationship between the predictor variables and outcome variable are linear and whether there is equal variance along the regression line. This seems to be a good model since the line if fairly straight.

```{r fig6, fig.path="figures/"}
my_model6 <- lm(Points ~ attitude, data = learning2014)
par(mfrow = c(2,2))
plot(my_model6, which = c(2))
```

**Normal QQ-plot** is a differnt type of scatter plot that looks at the distibution of the dependent and indepedent variable. If they are similarly normally distributed the dots of the plot should roughly form a line. If the distributions are not normally distributed the dots will take on a different shape (i.e. curved). In my model most of the points are near the line but deviations from the line can be seen on both the low and high ends. 

```{r fig7, fig.path="figures/"}
my_model7 <- lm(Points ~ attitude, data = learning2014)
par(mfrow = c(2,2))
plot(my_model7, which = c(5))
```

**Residuals vs Leverage plot** helps to identify influential data points on the model, such as outliers. The line of my model is fairly flat but you can see the majority of the data points are on the left.


